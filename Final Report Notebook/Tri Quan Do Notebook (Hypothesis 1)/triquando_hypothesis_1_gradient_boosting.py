# -*- coding: utf-8 -*-
"""TriQuanDo_Hypothesis_1_Gradient_Boosting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VcLXm-RjUTSrGl4Mb_l59in8m6rJqEsD
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt
import io, time, json
import requests
from bs4 import BeautifulSoup

import pandas as pd

url = "https://data.lacity.org/api/views/63jg-8b9z/rows.csv?accessType=DOWNLOAD"
los_crime_df = pd.read_csv(url)

# Show the first few rows of the DataFrame
print(los_crime_df.head())

# check the data
chicago_crime_df = pd.read_csv('ChicagoCrimes.csv')
print(chicago_crime_df.columns)

# we see that the column names 'Crm Cd Desc' and 'Premis Desc'
# is confusing so we rename them to their equivalent
los_crime_df = los_crime_df.rename(columns={
    'Crm Cd Desc': 'Primary Type',
    'Premis Desc': 'Location Description'
})
los_crime_df

def describe(df):
    '''
    Give some preliminary observation on the given dataframe:
    - Size and data type
    - NaN count each column
    - Value count each column
    '''
    print('**************************')
    print('*** Size and data type')
    size = len(df)
    print(f'Size: {df.size}')
    print(df.dtypes)
    print('**************************')
    print('*** NaN count each column')
    nan_df = df.isna().sum().to_frame(name='count')
    nan_df['percent'] = nan_df['count'] / size * 100
    print(nan_df)
    for col in df:
        print('**************************')
        print(f'*** Value count of {col}')
        val_df = df[col].value_counts().to_frame(name='count')
        val_df['percent'] = val_df['count'] / size * 100
        print(val_df)

describe(chicago_crime_df)

# column Location Description has very few row with NaN value
# we can drop that
chicago_crime_df = chicago_crime_df[~chicago_crime_df['Location Description'].isna()]
describe(chicago_crime_df)

describe(los_crime_df)

# Convert the 'Date' column to datetime format
chicago_crime_df['Date'] = pd.to_datetime(chicago_crime_df['Date'])

# Create a boolean mask to filter for data from 2010-2019 in Chicago
chicago_mask = (chicago_crime_df['Date'].dt.year >= 2010) & (chicago_crime_df['Date'].dt.year <= 2019)

# Apply the boolean mask to the DataFrame to filter the data
chicago_crime_df = chicago_crime_df.loc[chicago_mask]

# Print the resulting DataFrame
print(chicago_crime_df)

# Create a dictionary with the GDP data
chicago_GDP = {'Year': [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019],
            'GDP (billions of current dollars)': [591.4, 619.4, 642.9, 670.6, 699.6, 723.7, 734.9, 764.4, 795.2, 825.4]}

# Create a pandas DataFrame from the dictionary
chicago_GDP = pd.DataFrame(chicago_GDP)

# Print the resulting DataFrame
print(chicago_GDP)

# Create a dictionary with the GDP data
LA_GDP = {'Year': [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019],
            'GDP (billions of current dollars)': [703.8, 728.1, 747.8, 775.0, 805.6, 842.3, 870.8, 908.4, 944.6, 972.1]}

# Create a pandas DataFrame from the dictionary
LA_GDP = pd.DataFrame(LA_GDP)

# Print the resulting DataFrame
print(LA_GDP)

def plot_by_year(year_type_df, value_name):
    by_year = year_type_df[year_type_df['Primary Type'] == value_name].groupby(['Year']).count()
    plt.plot(by_year)
    plt.title(f'{value_name} count vs Year')
    plt.xlabel('Year')
    plt.ylabel(f'{value_name} count')
    plt.show()

year_type_df = chicago_crime_df[['Year', 'Primary Type']]

# show how much theft happens over time
plot_by_year(year_type_df, 'THEFT')

plot_by_year(year_type_df, 'BURGLARY')

plot_by_year(year_type_df, 'ROBBERY')

count_by_year = year_type_df.groupby(['Year']).count()
plt.plot(count_by_year)
plt.title(f'Crime count vs Year')
plt.xlabel('Year')
plt.ylabel(f'Crime count')
plt.show()

print("Column names in the DataFrame:")
print(chicago_crime_df.columns)

"""## Gradient Boosting Algorithm (Chicago Dataset): """

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_squared_error

# Assuming chicago_crime_df and chicago_GDP DataFrames are already loaded

# Make a copy of the original DataFrame
chicago_crime_copy = chicago_crime_df.copy()

# Preprocess the data on the copied DataFrame
chicago_crime_copy = chicago_crime_copy.loc[chicago_crime_copy['Year'].between(2010, 2019) & 
                                            chicago_crime_copy['Primary Type'].isin(['BURGLARY', 'ROBBERY', 'THEFT'])]
chicago_crime_copy = chicago_crime_copy.groupby(['Year', 'Primary Type']).size().reset_index(name='Count')
chicago_crime_copy = pd.pivot_table(chicago_crime_copy, values='Count', index=['Year'], columns=['Primary Type']).reset_index()
chicago_crime_copy.columns.name = None

# Compute crime rates as a percentage
chicago_crime_copy['Total'] = chicago_crime_copy[['BURGLARY', 'ROBBERY', 'THEFT']].sum(axis=1)
chicago_crime_copy['BURGLARY Rate'] = chicago_crime_copy['BURGLARY'] / chicago_crime_copy['Total'] * 100
chicago_crime_copy['ROBBERY Rate'] = chicago_crime_copy['ROBBERY'] / chicago_crime_copy['Total'] * 100
chicago_crime_copy['THEFT Rate'] = chicago_crime_copy['THEFT'] / chicago_crime_copy['Total'] * 100
chicago_crime_copy = chicago_crime_copy[['Year', 'BURGLARY Rate', 'ROBBERY Rate', 'THEFT Rate']]

# Merge the chicago_GDP DataFrame with the processed chicago_crime_copy DataFrame
merged_df = pd.merge(chicago_GDP, chicago_crime_copy, on='Year')
merged_df = merged_df.dropna()

# Split the data into training and testing sets
train_size = int(merged_df.shape[0] * 0.8)
train_df = merged_df[:train_size]
test_df = merged_df[train_size:]

# Train the Gradient Boosting model
X_train = train_df['GDP (billions of current dollars)'].values.reshape(-1, 1)
y_train = train_df[['BURGLARY Rate', 'ROBBERY Rate', 'THEFT Rate']].values
X_test = test_df['GDP (billions of current dollars)'].values.reshape(-1, 1)
y_test = test_df[['BURGLARY Rate', 'ROBBERY Rate', 'THEFT Rate']].values

gb_model = GradientBoostingRegressor()
multioutput_model = MultiOutputRegressor(gb_model)
multioutput_model.fit(X_train, y_train)

# Evaluate the model
y_pred = multioutput_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
gdp_crime_df = merged_df[['GDP (billions of current dollars)', 'BURGLARY Rate', 'ROBBERY Rate', 'THEFT Rate']]
corr_matrix = gdp_crime_df.corr()
print(corr_matrix)
# Plot the results
fig, ax = plt.subplots(figsize=(10, 6))
ax.scatter(X_test, y_test[:, 0], label='Burglary Rate')
ax.scatter(X_test, y_test[:, 1], label='Robbery Rate')
ax.scatter(X_test, y_test[:, 2], label='Theft Rate')
ax.scatter(X_test, y_pred[:, 0], label='Predicted Burglary Rate', marker='x')
ax.scatter(X_test, y_pred[:, 1], label='Predicted Robbery Rate', marker='x')
ax.scatter(X_test, y_pred[:, 2], label='Predicted Theft Rate', marker='x')
ax.set_xlabel('GDP (billions of current dollars)')
ax.set_ylabel('Crime Rate (%)')
ax.legend()
plt.show()

rmse = np.sqrt(mse)
print(f"Root Mean Squared Error: {rmse}")

print("Column names in the DataFrame:")
print(los_crime_df.columns)

"""## Conclusion of the Gradient Boosting Algorithm: 

_ Chicago Dataset: The correlation matrix shows that there is a strong negative correlation (-0.978) between GDP and the burglary rate, a moderate negative correlation (-0.289) between GDP and the robbery rate, and a strong positive correlation (0.949) between GDP and the theft rate. These results suggest that as GDP increases, the burglary rate tends to decrease, while the theft rate tends to increase. The correlation between GDP and the robbery rate is not as strong, suggesting that other factors may play a larger role in determining the incidence of robbery.
"""